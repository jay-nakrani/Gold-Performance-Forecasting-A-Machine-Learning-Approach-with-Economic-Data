# -*- coding: utf-8 -*-
"""gold_performance_forecasting_weekly.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_8Y39mkEDtMH167cZIC141hkZ08ydayw

# Gold Performance Forecasting: A Machine Learning Approach with Economic Data

# Step-1 : *Data collection, analysis and cleaning.*
"""

# To install required packages and import libraries.
import yfinance as yf
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime

# Function to get the data.
def get_data(start='2000-01-01'):
    """
    To download all the required data from Yahoo finance using yfinance-
    library.(From 2000 to Today)

    parameters: start= start date of data.

    returns: dictionary of all the downloaded data.
    """

    # To download all the required data from Yahoo finance using yfinance-
    # library.(From 2000 to Today)
    # Gold futures.
    gold = yf.download('GC=F', start=start, progress=False)
    # Treasury yield.(thirty years treasury yield)
    treasury_yield = yf.download('^TYX', start=start, progress=False)
    # USD Index.
    usd_index = yf.download('DX=F', start=start, progress=False)
    # Volatility Index (VIX).
    vix = yf.download('^VIX', start=start, progress=False)
    # US oil futures.
    oil = yf.download('CL=F', start=start, auto_adjust=True)
    # Silver futures.
    silver = yf.download('SI=F', start=start, progress=False)

    # To download inflation data (CPI) from federal reserve economic data (FRED).
    cpi_url = "https://fred.stlouisfed.org/graph/fredgraph.csv?id=CPIAUCSL"
    cpi_data = pd.read_csv(cpi_url, index_col=0, parse_dates=True)
    cpi_data.columns = ['CPI']

    # To keep CPI data from year 1999 onwards.
    # # All other data starts from year 2000 but here we need one more year to
    # # calculate anual inflation for year 2000.

    # Convert start to datetime.
    start_date = pd.to_datetime(start)

    # Calculate CPI start date (1 year before start date).
    cpi_start_date = start_date - pd.DateOffset(years=1)
    cpi_start_str = cpi_start_date.strftime('%Y-%m-%d')

    cpi_data = cpi_data[cpi_data.index >= cpi_start_str]

    # To calculate annual inflation rate.
    # For example: march,2024-march,2025; april,2024- april,2025 etc..
    cpi_data['Inflation_Annual'] = cpi_data['CPI'].pct_change(12) * 100  # Year-over-year

    # To keep CPI data from 2000-01-01 onwards for analysis as all other data is from
    # year 2000.
    cpi_data_final = cpi_data[cpi_data.index >= start]

    # Resample to daily frequency with forward fill.
    inflation_annual_daily = cpi_data_final['Inflation_Annual'].resample('D').ffill()

    return {
        'gold': gold,
        'treasury_yield': treasury_yield,
        'usd_index': usd_index,
        'vix': vix,
        'oil': oil,
        'silver': silver,
        'inflation_annual': inflation_annual_daily
    }

dataframe = get_data(start='2000-01-01')

# To check each downloaded data.
datasets_to_check = {
    'Gold': dataframe['gold'],
    'Treasury_yield': dataframe['treasury_yield'],
    'USD Index': dataframe['usd_index'],
    'VIX': dataframe['vix'],
    'US Oil': dataframe['oil'],
    'Silver': dataframe['silver'],
    'Inflation Annual Daily': dataframe['inflation_annual']
}

for name, data in datasets_to_check.items():
    print(f"\n=== {name.upper()} ===")
    print(f"Shape: {data.shape}")
    print(f"Date Range: {data.index[0]} to {data.index[-1]}")
    print("\nFirst 3 rows:\n")
    print(data.tail(3))
    print("\nData types:\n")
    print(data.dtypes)
    print(f"Missing values: {data.isnull().sum().sum()}")

    print("-" * 50)
    print("-" * 50)

# To check if all the data is in same range or not.
print("\nDATE RANGE COMPARISON")
print("=" * 50)

for name, data in datasets_to_check.items():

    # String representation used to avoid errors.
    start_date = str(data.index[0])[:10]  # Get first 10 characters (YYYY-MM-DD)
    end_date = str(data.index[-1])[:10]

    print(f"{name:25} | {start_date} to {end_date} | {len(data):6} records")

# function to combine all the data we collected with it's daily close.

def combine_market_data(data):

    """
    This function helps to create a combined dataset of all collected data with-
    it's daily close.

    Parameters: data (dict): A dictionary containing different market datasets.

    Returns: combined_data (pandas.DataFrame): A combined dataset with all-
             features aligned to daily frequency.

    """
    # Now we can combine all the datasets into one dataset. we require only close
    # price from each day for further analysis.

    # create a copy to avoid modifying original dataframe.
    data = data.copy()

    # To initialize combined dataframe.
    combined_data = pd.DataFrame()

    # To extract close prices from each dataset.
    combined_data['Gold_Price'] = data['gold'][('Close', 'GC=F')]
    combined_data['Treasury_yield'] = data['treasury_yield'][('Close', '^TYX')]
    combined_data['USD_Index'] = data['usd_index'][('Close', 'DX=F')]
    combined_data['VIX'] = data['vix'][('Close', '^VIX')]
    combined_data['OIL'] = data['oil'][('Close', 'CL=F')]
    combined_data['SILVER'] = data['silver'][('Close', 'SI=F')]

    # Inflation annual has different end date than other data.
    # Inflation data we get every month but it is for the last month. For example
    # current month is october so we get the inflation data of september in the
    # starting of october. This data don't change daily so we can forward fill it
    # with the last known value.
    # So here we will forward fill it till last date of gold dataset.
    target_end = data['gold'].index[-1]

    inflation_annual_extended = data['inflation_annual'].reindex(
        pd.date_range(start=data['inflation_annual'].index[0], end=target_end, freq='D')
        ).ffill()

    # Inflation annual has only one value in dataset so we do not need to
    # extract close price from it.
    combined_data['Inflation_Annual'] = inflation_annual_extended

    # Remove any missing values from the combined dataset.
    combined_data = combined_data.dropna()

    return combined_data

combined_df = combine_market_data(dataframe)

print(f"Shape: {combined_df.shape}")
print(f"Date range: {combined_df.index[0]} to {combined_df.index[-1]}")
print("\nCombined data information")
print("=" * 40 + "\n")
print(combined_df.info())
print("\nMissing values:")
print(combined_df.isnull().sum())
print("\nBasic statistics:")
print(combined_df.describe())

"""# Step-2 : *Data visualization.*"""

# Function to plot all the features collected in combined dataframe.

def plot_combined_data(combined_df):

    """
    This function helps to plot all the features fromk the dataframe.

    Parameters: combined_df: A combined dataset with all-
                features aligned to daily frequency.

    Returns: None
    """

    # To create a list of features available in the dataframe.
    available_columns = combined_df.columns.tolist()

    # To initialize subplots.
    fig, axes = plt.subplots(4, 2, figsize=(20,30)) # 4 rows, 2 columns, total 8 subplots.
    fig.suptitle('All features from combined dataset\n', fontsize=24, fontweight='bold')
    axes = axes.flatten() # To create 1D array to iterate through subplots easily.

    # To define plot configurations for each variable.
    plot_config = {
        'Gold_Price': {'color': 'gold', 'title': 'Gold Price', 'ylabel': 'Price ($)'},
        'SILVER': {'color': 'darkgray', 'title': 'Silver Price', 'ylabel': 'Price ($)'},
        'OIL': {'color': 'black', 'title': 'Oil Price', 'ylabel': 'Price ($)'},
        'Treasury_yield': {'color': 'red', 'title': 'Treasury Yield', 'ylabel': 'Yield (%)'},
        'USD_Index': {'color': 'green', 'title': 'USD Index', 'ylabel': 'Index Value'},
        'VIX': {'color': 'purple', 'title': 'VIX (Volatility)', 'ylabel': 'VIX Level'},
        'Inflation_Annual': {'color': 'blue', 'title': 'Annual Inflation', 'ylabel': 'Inflation (%)'}
    }

    # To plot subplots for each variables.
    for i, column in enumerate(available_columns):
        if i < len(axes):
            config = plot_config[column]
            axes[i].plot(combined_df.index, combined_df[column], color=config['color'], linewidth=1.5)
            axes[i].set_title('\n' + config['title'], fontweight='bold', fontsize=18)
            axes[i].set_ylabel(config['ylabel'], fontsize=12, fontweight='bold')
            axes[i].set_xlabel('Year', fontsize=12, fontweight='bold')
            axes[i].grid(False)
            axes[i].tick_params(axis='x', rotation=45)

    # To hide unused subplot.
    for i in range(len(available_columns), len(axes)):
        axes[i].set_visible(False)

    plt.tight_layout()
    plt.show()

plot_combined_data(combined_df)

# Function to visualize the correlation between gold and inflation annual.

def plot_gold_inflation_correlation(df):

    """
    This function helps to plot 1-year rolling correlation between Gold Price
    and Inflation Annual.

    Parameters: df = dataframe.
    """

    # To calculate 1-year rolling correlation between gold and inflation.
    # window=252 as there are roughly 252 trading days in a year.
    rolling_corr = df['Gold_Price'].rolling(window=252).corr(df['Inflation_Annual'])

    # To create the correlation plot.
    plt.figure(figsize=(14, 8))
    plt.plot(rolling_corr.index, rolling_corr, color='purple', linewidth=2.5, label='Gold-Inflation Correlation')

    # To add shaded regions for positive/negative correlation.
    plt.fill_between(rolling_corr.index, rolling_corr, 0, where=rolling_corr >= 0,
                     color='green', alpha=0.2, label='Positive Correlation')
    plt.fill_between(rolling_corr.index, rolling_corr, 0, where=rolling_corr <= 0,
                     color='red', alpha=0.2, label='Negative Correlation')

    plt.axhline(y=0, color='black', linestyle='--', alpha=0.7, linewidth=1)
    plt.title('1-Year Rolling Correlation: Gold Price vs Inflation\n', fontsize=18, fontweight='bold')
    plt.ylabel('Correlation Coefficient', fontsize=12, fontweight='bold')
    plt.xlabel('Date', fontsize=12, fontweight='bold')
    plt.grid(True, alpha=0.4)
    plt.legend()
    plt.tight_layout()
    plt.show()

plot_gold_inflation_correlation(combined_df)

# Function to visualize gold behaviour during high inflation.

def plot_gold_vs_inflation(df):

    """
    This function helps to plot gold prices during high inflation periods.

    Parameters: df = Combined dataset with financial variables

    Returns: high_inflation_periods = DataFrame containing high inflation periods
    """

    high_inflation_threshold = df['Inflation_Annual'].quantile(0.70)
    high_inflation_periods = df[df['Inflation_Annual'] >= high_inflation_threshold]

    plt.figure(figsize=(12, 6))
    plt.plot(df.index, df['Gold_Price'], color='darkgray', label='Gold_price')
    scatter = plt.scatter(high_inflation_periods.index, high_inflation_periods['Gold_Price'],
                         c=high_inflation_periods['Inflation_Annual'], cmap='plasma_r',
                         s=20, alpha=0.7, label='High Inflation Periods')

    # To set the colorbar to threshold.
    vmin = high_inflation_threshold
    vmax = high_inflation_periods['Inflation_Annual'].max()
    scatter.set_clim(vmin=vmin, vmax=vmax)
    cbar = plt.colorbar(scatter)
    cbar.set_label('Inflation Rate (%)')
    ticks = [vmin] + list(np.linspace(vmin, vmax, 6)[1:])
    cbar.set_ticks(ticks)
    cbar.set_ticklabels([f"{t:.2f}" for t in ticks])

    plt.title('Gold prices during high inflation periods\n', fontsize=18, fontweight='bold')
    plt.ylabel('Price', fontsize=12, fontweight='bold')
    plt.xlabel('Date', fontsize=12, fontweight='bold')
    plt.legend()
    plt.xticks(pd.date_range(start=df.index.min(), end=df.index.max(), freq='4YS'))
    plt.show()

    return high_inflation_periods

high_inflation_periods = plot_gold_vs_inflation(combined_df)

# Function to plot correlation heatmap.

def plot_correlation_heatmap(df, figsize):

    """
    This function helps to plot correlation heatmap for the dataset.

    Parameters: df = dataframe.
                figsize = tuple, Figure size for the heatmap.

    """

    # To calculate correlation matrix of the dataframe.
    corr_matrix = df.corr()

    # To create heatmap using correlation matrix.
    plt.figure(figsize=figsize)

    # To mask the upper triangle for cleaner output.
    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))

    sns.heatmap(corr_matrix, mask=mask, annot=True, cmap='coolwarm', center=0,
                fmt='.3f')

    plt.title('Correlation matrix', fontsize=18, fontweight='bold')
    plt.tight_layout()
    plt.show()

corr_matrix = plot_correlation_heatmap(combined_df,(10,8))

# To check the correlation of all the features with gold.
print("\ncorrelation with gold")
print("=" * 35)
daily_c = combined_df.pct_change().dropna()
correlations = daily_c.corr()['Gold_Price'].sort_values(ascending=False)
print(correlations)

# Function to plot correlation heatmap.

def plot_correlation_heatmap(df, figsize):

    """
    This function helps to plot correlation heatmap for the dataset.

    Parameters: df = dataframe.
                figsize = tuple, Figure size for the heatmap.

    """

    # To calculate correlation matrix of the dataframe.
    corr_matrix = df.corr()

    # To create heatmap using correlation matrix.
    plt.figure(figsize=figsize)

    # To mask the upper triangle for cleaner output.
    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))

    sns.heatmap(corr_matrix, mask=mask, annot=True, cmap='coolwarm', center=0,
                fmt='.3f')

    plt.title('Correlation matrix', fontsize=18, fontweight='bold')
    plt.tight_layout()
    plt.show()

corr_matrix = plot_correlation_heatmap(daily_c,(10,8))

"""# Step-3 : *Machine Learning.*

* Aim for this machine learning project is to predict gold direction
on weekly basis.
* Model is trained on major (+ and -) correlated features of cobined_df dataframe and some technical indicators which I have created in below functions.
"""

# To install required packages and import libraries.

import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.linear_model import RidgeClassifier
from sklearn.model_selection import GridSearchCV, TimeSeriesSplit, RandomizedSearchCV
from sklearn.metrics import (accuracy_score, precision_score, recall_score,
                             f1_score, confusion_matrix,
                             balanced_accuracy_score)
from sklearn.feature_selection import SelectKBest, f_classif
import warnings
warnings.filterwarnings('ignore')

# To create features to train the model.

def create_features(df):

    """
    This function helps to create weekly prediction features from daily data.
    Features are created using only data available up to each Friday.

    Parameters: df = Combined dataset with financial variables

    Returns: data = DataFrame with weekly prediction features
    """

    # To make a copy of the orignal dataframe to avoid modifying it.
    data = df.copy()

    # To creat economic feature of real yield.
    # Real yield calculation based on Federal Reserve research: Gürkaynak, Sack, & Wright (2008)
    data['Real_Yield'] = data['Treasury_yield'] - data['Inflation_Annual']
    data['Real_Yield_Change_5d'] = data['Real_Yield'] - data['Real_Yield'].shift(5)
    data['Real_Yield_Change_10d'] = data['Real_Yield'] - data['Real_Yield'].shift(10)

    # # To create features based on VIX.
    # #
    # To create vix level feature based on VIX prices. (fear level)
    # Low: < 15, Medium: 15-30, High: > 30
    vix_level = pd.cut(data['VIX'], bins=[0, 15, 30, 100], labels=[0, 1, 2])
                                                    # 0=Low, 1=Medium, 2=High
    data['VIX_Level'] = vix_level.astype(float)

    # To create features of percentage changes in price of vix in last 10
    # days.
    data['VIX_Change_10d'] = data['VIX'].pct_change(10)

    # To create feature of vix's current price vs. it's 10 days avrage price.
    data['VIX_vs_10d_Avg'] = data['VIX'] / data['VIX'].rolling(10).mean() - 1

    # # To create features based on ratio of last 5 days between gold and silver,
    # # gold and oil.
    Gold_silver = data['Gold_Price'] / data['SILVER']
    Gold_oil = data['Gold_Price'] / data['OIL']
    Gold_dxy = data['Gold_Price'] / data['USD_Index']
    Gold_yield = data['Gold_Price'] / data['Treasury_yield']
    Gold_ryield = data['Gold_Price'] / data['Real_Yield']

    data['Gold_Silver_Ratio_5d_Avg'] = Gold_silver.rolling(5).mean()
    data['Gold_Yield_Ratio_5d_Avg'] = Gold_yield.rolling(5).mean()
    data['Gold_Real_Yield_Ratio_5d_Avg'] = Gold_ryield.rolling(5).mean()

    # To also get that ratio in percentage.
    data['Gold_DXY_Ratio_Change_5d'] = Gold_dxy.pct_change(5)

    # # To create features based on CPI data (inflation).
    # #
    # Inflation data is publised on monthly basis so we need to detect when it
    # changes in the data.
    # To create a flag for new inflation data.
    data['Inflation_Change_Flag'] = (data['Inflation_Annual'] != data['Inflation_Annual'].shift(1)).astype(int)

    # To create a feature for the monthly inflation change (current vs previous month).
    inflation_changes = data[data['Inflation_Change_Flag'] == 1]['Inflation_Annual'].diff()

    # To forward fill the change values to all days until next update.
    data['Inflation_Actual_Change'] = inflation_changes.reindex(data.index).ffill()

    # # To create features for the 10-day rolling correlation between gold and
    # # DXY, yield, and silver.
    # Gold vs DXY relationship.
    data['Gold_DXY_Corr_10d'] = data['Gold_Price'].pct_change().rolling(10).corr(data['USD_Index'].pct_change())

    # Gold vs Yield relationship.
    data['Gold_Yield_Corr_10d'] = data['Gold_Price'].pct_change().rolling(10).corr(data['Treasury_yield'].pct_change())

    # Gold vs Silver correlation.
    data['Gold_Silver_Corr_10d'] = data['Gold_Price'].pct_change().rolling(10).corr(data['SILVER'].pct_change())

    # To remove unnecessary features.
    data = data.drop(['Inflation_Change_Flag'], axis=1, errors='ignore')

    # To remove NaN values from the data. (created during rolling calculations)
    initial_len = len(data)
    data = data.dropna()
    final_len = len(data)

    print(f"Removed {initial_len - final_len} rows with NaN")
    print(f"Final dataset shape: {data.shape}")
    print(f"Date range after cleaning: {data.index.min()} to {data.index.max()}")
    print(f"Number of features: {len(data.columns)}")

    return data

features_df = create_features(combined_df)

def filter_to_fridays(df):

    """
    This function helps to filter data to Fridays only.

    Parameters: df = Combined dataset with financial variables

    Returns: friday_data = DataFrame with only Fridays

    """

    # To get only Fridays from the data and sort it by date.
    friday_data = df[df.index.dayofweek == 4].copy()
    friday_data = friday_data.sort_index()

    # To get the gold's next week return.
    gold_returns = friday_data['Gold_Price'].pct_change().shift(-1)

    # To create the binary target: 1 if next week's return > 0, else 0. (direction)
    friday_data['Target_Next_Week_Up'] = (gold_returns > 0).astype(int)

    # Here we are working with the current data so we don't have next week's
    # price. So the above direction feature will create a Nan value in the end.
    # To remove the last row.
    friday_data = friday_data.iloc[:-1]

    print(f"Friday Data: {len(friday_data)} weeks")
    print(f"Target: {friday_data['Target_Next_Week_Up'].mean():.1%} Up rate")

    return friday_data

friday_features = filter_to_fridays(features_df)

def prepare_training_data(friday_df):

    """
    This function helps to prepare the data for training.

    Parameters: friday_df = DataFrame with only Fridays.

    Returns: X, y, feature_cols
    """

    # To exclude the raw prices and next week's direction to avoid data leakage.
    exclude_cols = ['Gold_Price', 'SILVER', 'OIL', 'USD_Index', 'VIX',
                   'Treasury_yield', 'Inflation_Annual', 'Target_Next_Week_Up']
    feature_cols = [col for col in friday_df.columns if col not in exclude_cols]

    X = friday_df[feature_cols]
    y = friday_df['Target_Next_Week_Up']

    print(f"Prepared training data:\n")
    print(f"X shape: {X.shape}")
    print(f"y shape: {y.shape}")
    print(f"Number of features: {len(feature_cols)}")
    print(f"Target distribution: {y.value_counts().to_dict()}")

    return X, y, feature_cols

X, y, feature_cols = prepare_training_data(friday_features)

def time_based_split(X, y, test_size):

    """
    This function helps to split data into train and test.

    Parameters: X = DataFrame with features.
                y = target.
                test_size = float, Proportion of data to be used for testing.

    Returns: X_train = DataFrame with training features.
             X_test = DataFrame with testing features.
             y_train = Training targets.
             y_test = Testing targets.
    """

    split_point = int(len(X) * (1 - test_size))

    X_train = X.iloc[:split_point]
    X_test = X.iloc[split_point:]
    y_train = y.iloc[:split_point]
    y_test = y.iloc[split_point:]

    print(f"\nTime-based split:")
    print(f"Training period: {X_train.index.min().date()} to {X_train.index.max().date()}")
    print(f"Testing period: {X_test.index.min().date()} to {X_test.index.max().date()}")
    print(f"Train samples: {len(X_train)}")
    print(f"Test samples: {len(X_test)}")

    return X_train, X_test, y_train, y_test

X_train, X_test, y_train, y_test = time_based_split(X, y, test_size=0.2)

def scale_features(X_train, X_test):

    """
    This function helps to normalize features using standardScaler.

    Parameters: X_train = DataFrame with training features.
                X_test : DataFrame with test features.

    Returns: X_train_scaled = DataFrame with scaled training data.
             X_test_scaled = DataFrame with scaled test data.
             scaler = Standardscaler with trained scaler object.
    """

    # To initialize the scaler.
    scaler = StandardScaler()

    # To fit the scaler on training data only and transform it to both traning
    # and testing data. on training data and transform both.
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # To convert scaled arrays back to DataFrames with original column names and
    # dates. StandardScaler.fit_transform() returns numpy arrays, losing feature
    # names and timestamps. We need DataFrames for proper model training and
    # evaluation
    X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns,
                                  index=X_train.index)
    X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns,
                                  index=X_test.index)

    print(f"Features scaled using StandardScaler")

    return X_train_scaled, X_test_scaled, scaler

X_train_scaled, X_test_scaled, scaler = scale_features(X_train, X_test)

"""**Training and testing models**"""

def perform_time_series_cv(model, X_train, y_train, model_name="Model", n_splits=5):

    """
    This function performs time series cross-validation for any model and returns
    the cross-validation scores.

    Parameters: model : Any machine learning model.
                X_train : DataFrame with training features.
                y_train : Series with training target values.
                model_name : Name of the model for display purposes.
                n_splits : Number of folds for cross-validation.

    Returns: cv_scores : Dictionary with cross-validation metrics.

    """
    print(f"\n Performing {n_splits}-fold Time Series Cross-Validation for {model_name}:")

    # To configure time series cross-validation.
    tscv = TimeSeriesSplit(n_splits=n_splits)

    # To store cross-validation metrics for each fold.
    cv_train_accs = []
    cv_val_accs = []
    cv_f1_scores = []
    cv_bal_accs = []
    cv_precisions = []
    cv_recalls = []

    # To iterate through each fold of cross-validation.
    for fold, (train_i, val_i) in enumerate(tscv.split(X_train), 1):

        # To split data into training and validation for current fold.
        X_tr = X_train.iloc[train_i]
        X_val = X_train.iloc[val_i]
        y_tr = y_train.iloc[train_i]
        y_val = y_train.iloc[val_i]

        # To train model on training fold.
        model.fit(X_tr, y_tr)

        # To make predictions on training and validation data.
        y_tr_pred = model.predict(X_tr)
        y_val_pred = model.predict(X_val)

        # To calculate performance metrics for current fold.
        train_acc = accuracy_score(y_tr, y_tr_pred)
        val_acc = accuracy_score(y_val, y_val_pred)
        f1 = f1_score(y_val, y_val_pred, zero_division=0)
        bal_acc = balanced_accuracy_score(y_val, y_val_pred)
        precision = precision_score(y_val, y_val_pred, zero_division=0)
        recall = recall_score(y_val, y_val_pred, zero_division=0)

        # To store metrics for current fold.
        cv_train_accs.append(train_acc)
        cv_val_accs.append(val_acc)
        cv_f1_scores.append(f1)
        cv_bal_accs.append(bal_acc)
        cv_precisions.append(precision)
        cv_recalls.append(recall)

        print(f"   Fold {fold}/{n_splits}: Train={train_acc:.4f}, Val={val_acc:.4f}, "
                  f"BalAcc={bal_acc:.4f}, F1={f1:.4f}")

    # To calculate mean performance across all folds.
    mean_train = np.mean(cv_train_accs)
    mean_val = np.mean(cv_val_accs)
    mean_f1 = np.mean(cv_f1_scores)
    mean_bal = np.mean(cv_bal_accs)
    mean_precision = np.mean(cv_precisions)
    mean_recall = np.mean(cv_recalls)
    std_val = np.std(cv_val_accs)
    gap = mean_train - mean_val

    # To print cross-validation summary results.
    print("\n Cross-Validation Summary:")
    print(f"    Mean Train Accuracy:     {mean_train:.4f}")
    print(f"    Mean Val Accuracy:       {mean_val:.4f} (±{std_val:.4f})")
    print(f"    Mean Balanced Accuracy:  {mean_bal:.4f}")
    print(f"    Mean F1 Score:           {mean_f1:.4f}")
    print(f"    Mean Precision:          {mean_precision:.4f}")
    print(f"    Mean Recall:             {mean_recall:.4f}")
    print(f"    Train-Val Gap:           {gap:.4f}", end=" ")

    # To provide interpretation of overfitting gap.
    if gap < 0.05:
        print(" Excellent generalization!")
    elif gap < 0.15:
        print(" Good generalization")
    else:
        print(" Overfitting detected")

    # To store cross-validation scores in dictionary.
    cv_scores = {
        'train_acc': mean_train,
        'val_acc': mean_val,
        'balanced_acc': mean_bal,
        'f1': mean_f1,
        'precision': mean_precision,
        'recall': mean_recall,
        'std': std_val,
        'gap': gap
    }

    return cv_scores

def evaluate_model_generic(model, X_train, X_test, y_train, y_test, model_name="Model"):

    """
    This function provides generic evaluation for any model.

    Parameters: model : Any sklearn model with fit/predict methods.
                X_train : DataFrame with training features.
                X_test : DataFrame with test features.
                y_train : Series with training target values.
                y_test : Series with test target values.
                model_name : Name of the model for display purposes.

    Returns: results : Dictionary containing all evaluation metrics.
    """

    print(f"\nModel evalution for {model_name.upper()}")

    # To get model predictions for training and test data.
    y_train_pred = model.predict(X_train)
    y_test_pred = model.predict(X_test)

    # To calculate various performance metrics.
    train_acc = accuracy_score(y_train, y_train_pred)
    test_acc = accuracy_score(y_test, y_test_pred)
    test_bal_acc = balanced_accuracy_score(y_test, y_test_pred)
    precision = precision_score(y_test, y_test_pred, zero_division=0)
    recall = recall_score(y_test, y_test_pred, zero_division=0)
    f1 = f1_score(y_test, y_test_pred, zero_division=0)

    # To print the calculated performance metrics.
    print("\n Performance Metrics:")
    print(f"    Training Accuracy:       {train_acc:.4f}")
    print(f"    Test Accuracy:           {test_acc:.4f}")
    print(f"    Balanced Accuracy:       {test_bal_acc:.4f}")
    print(f"    Precision:               {precision:.4f}")
    print(f"    Recall:                  {recall:.4f}")
    print(f"    F1 Score:                {f1:.4f}")

    # To check for overfitting by comparing training and test accuracy.
    gap = train_acc - test_acc
    print("\n Generalization Analysis:")
    print(f"   Train-Test Gap:          {gap:+.4f}", end=" ")

    # To provide interpretation of the overfitting gap.
    if gap < 0.05:
        print(" Excellent generalization!")
    elif gap < 0.15:
        print(" Good generalization")
    else:
        print(" Overfitting detected")

    # To create and display the confusion matrix.
    cm = confusion_matrix(y_test, y_test_pred)
    print("\n Confusion Matrix:")
    print("                     Predicted Down  Predicted Up")
    print(f"      Actual Down         {cm[0,0]:4d}          {cm[0,1]:4d}")
    print(f"      Actual Up           {cm[1,0]:4d}          {cm[1,1]:4d}")

    # To calculate per-class accuracy for detailed analysis.
    print("\n Per-Class Performance:")
    down_total = cm[0,0] + cm[0,1]
    up_total = cm[1,0] + cm[1,1]

    # To calculate accuracy for down weeks.
    if down_total > 0:
        down_acc = cm[0,0] / down_total
        print(f"    Down weeks:  {cm[0,0]:3d}/{down_total:3d} correct ({down_acc:.1%})")

    # To calculate accuracy for up weeks.
    if up_total > 0:
        up_acc = cm[1,1] / up_total
        print(f"    Up weeks:    {cm[1,1]:3d}/{up_total:3d} correct ({up_acc:.1%})")

    # Try to get feature importance if available
    feature_importance = None
    if hasattr(model, 'feature_importances_'):
        feature_importance = pd.DataFrame({
            'Feature': X_train.columns,
            'Importance': model.feature_importances_
        }).sort_values('Importance', ascending=False)
    elif hasattr(model, 'coef_'):
        # For linear models
        feature_importance = pd.DataFrame({
            'Feature': X_train.columns,
            'Coefficient': model.coef_[0] if len(model.coef_.shape) > 1 else model.coef_,
            'Abs_Coefficient': abs(model.coef_[0] if len(model.coef_.shape) > 1 else model.coef_)
        }).sort_values('Abs_Coefficient', ascending=False)

    # To store all results in a dictionary for further use.
    results = {
        'train_acc': train_acc,
        'test_acc': test_acc,
        'balanced_acc': test_bal_acc,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'gap': gap,
        'confusion_matrix': cm,
        'feature_importance': feature_importance}

    if feature_importance is not None:
        print(f"\n  Top 5 Most Important Features:")
        for i in range(min(5, len(feature_importance))):
            feat = feature_importance.iloc[i]
            if 'Coefficient' in feat:
                direction = "UP" if feat['Coefficient'] > 0 else "DOWN"
                print(f"   {i+1:2d}. {feat['Feature']:30s}: {feat['Coefficient']:+.4f} (predicts {direction})")
            else:
                print(f"   {i+1:2d}. {feat['Feature']:30s}: {feat['Importance']:.4f}")

    return results

def train_model_generic(model, X_train, y_train, X_test, y_test, model_name="Model"):

    """
    This is a complete generic training pipeline for any model.

    Parameters: model : Any sklearn model.
                X_train, y_train : Training data.
                X_test, y_test : Test data.
                model_name : Name for display.

    Returns: trained_model : Trained model.
             cv_scores : Cross-validation scores.
             results : Test evaluation results.
    """

    print(f"TRAINING {model_name.upper()}")
    print(f"{'='*80}")

    # To perform cross-validation on the model.
    cv_scores = perform_time_series_cv(model, X_train, y_train, model_name)

    # To train the model on full training set.
    print(f"\n Training Final Model on Full Training Set...")
    model.fit(X_train, y_train)
    print(f"    {model_name} training complete!")

    # To evaluate the model on test set.
    results = evaluate_model_generic(model, X_train, X_test, y_train, y_test, model_name)

    # To print the model performance summary.
    print(f"\n{model_name.upper()} SUMMARY")
    print(f"{'='*80}")
    print(f"    Test Accuracy:        {results['test_acc']:.4f}")
    print(f"    Balanced Accuracy:    {results['balanced_acc']:.4f}")
    print(f"    F1 Score:             {results['f1']:.4f}")
    print(f"    Precision:            {results['precision']:.4f}")
    print(f"    Recall:               {results['recall']:.4f}")
    print(f"    Overfitting Gap:      {results['gap']:.4f}")

    return model, cv_scores, results

"""- Logistic regression"""

def train_logistic_regression(X_train, y_train, X_test, y_test):

    """
    This function trains a Logistic Regression model.

    Parameters: X_train, y_train : Training data.
                X_test, y_test : Test data.

    Returns: model : Trained Logistic Regression model.
             cv_scores : Cross-validation scores.
             results : Test evaluation results.
    """

    # To initialize the model with specified parameters.
    model = LogisticRegression(C=0.01,
                               penalty='l2',
                               max_iter=1000,
                               random_state=42,
                               solver='lbfgs')

    # To use the generic training pipeline.
    model, cv_scores, results = train_model_generic(model, X_train, y_train,
                                                    X_test, y_test, "Logistic Regression")

    return model, cv_scores, results

model_lr, cv_lr, results_lr = train_logistic_regression(X_train_scaled, y_train,
                                                        X_test_scaled, y_test)

def tune_logistic_regression(X_train, y_train, X_test, y_test):

    """
    This function performs hyperparameter tuning for Logistic Regression.

    Parameters: X_train, y_train : Training data.
                X_test, y_test : Test data.
    Returns: best_model : Best Logistic Regression model.
             cv_scores : Cross-validation scores.
             results : Test evaluation results.
    """

    # To define the parameter grid for ALL parameters.
    param_grid = {'C': [0.011, 0.057, 0.113],
                  'penalty': ['l1', 'l2'],
                  'max_iter': [500, 1000],
                  'fit_intercept': [True],
                  'class_weight': [None, 'balanced', {0: 1, 1: 1.1}, {0: 1.1, 1: 1}],
                  'random_state': [42, 123],
                  'solver': ['lbfgs', 'newton-cg']}

    # To configure time series cross-validation.
    tscv = TimeSeriesSplit(n_splits=5)

    # To create GridSearchCV for hyperparameter tuning.
    grid_search = GridSearchCV(estimator=LogisticRegression(random_state=42),
                               param_grid=param_grid,
                               cv=tscv,
                               scoring='balanced_accuracy',
                               n_jobs=-1,
                               verbose=1,
                               refit=True)

    # To execute grid search.
    print(f"Tuning Logistic Regression...")
    grid_search.fit(X_train, y_train)

    # To get best model and ALL parameters.
    best_model = grid_search.best_estimator_
    best_params = grid_search.best_params_

    print(f"Best Parameters Found:")
    print(f"  C: {best_params.get('C')}")
    print(f"  penalty: {best_params.get('penalty')}")
    print(f"  max_iter: {best_params.get('max_iter')}")
    print(f"  fit_intercept: {best_params.get('fit_intercept')}")
    print(f"  class_weight: {best_params.get('class_weight')}")
    print(f"  random_state: {best_params.get('random_state')}")
    print(f"  solver: {best_params.get('solver')}")
    print(f"Best CV Score: {grid_search.best_score_:.4f}")

    # To use the generic training pipeline with tuned model.
    model, cv_scores, results = train_model_generic(best_model, X_train, y_train,
                                                    X_test, y_test, "Tuned Logistic Regression")

    return model, cv_scores, results

results_dict = {}

lr_model, lr_cv, lr_results = tune_logistic_regression(X_train, y_train, X_test, y_test)
results_dict['Logistic Regression'] = {'model': lr_model,
                                       'cv_scores': lr_cv,
                                       'results': lr_results}

"""- Ridge classifier"""

def train_ridge_classifier(X_train, y_train, X_test, y_test):

    """
    This function trains a Ridge Classifier model.

    Parameters: X_train, y_train : Training data.
                X_test, y_test : Test data.

    Returns: model : Trained Ridge Classifier model.
             cv_scores : Cross-validation scores.
             results : Test evaluation results.
    """

    # To initialize the Ridge Classifier model.
    model = RidgeClassifier(alpha=10,
                            solver='svd',
                            random_state=42,
                            class_weight='balanced')

    # To use the generic training pipeline.
    model, cv_scores, results = train_model_generic(model, X_train, y_train,
                                                    X_test, y_test, "Ridge Classifier")

    return model, cv_scores, results

model_ridge, cv_ridge, results_ridge = train_ridge_classifier(X_train_scaled, y_train,
                                                              X_test_scaled, y_test)

def tune_ridge_classifier(X_train, y_train, X_test, y_test):

    """
    This function performs hyperparameter tuning for Ridge Classifier.

    Parameters: X_train, y_train : Training data.
                X_test, y_test : Test data.
    Returns: best_model : Best Ridge Classifier model.
             cv_scores : Cross-validation scores.
             results : Test evaluation results.
    """

    # To define the parameter grid for ALL parameters.
    param_grid = {'alpha': [50, 100, 500],
                  'solver': ['svd', 'cholesky'],
                  'fit_intercept': [True, False],
                  'copy_X': [True, False],
                  'random_state': [42, 123, 456, 789],
                  'class_weight': [None, 'balanced', {0: 1, 1: 1.1}, {0: 1.1, 1: 1}]}

    # To configure time series cross-validation.
    tscv = TimeSeriesSplit(n_splits=5)

    # To create GridSearchCV for hyperparameter tuning.
    grid_search = GridSearchCV(estimator=RidgeClassifier(random_state=42),
                               param_grid=param_grid,
                               cv=tscv,
                               scoring='balanced_accuracy',
                               n_jobs=-1,
                               verbose=1,
                               refit=True)

    # To execute grid search.
    print(f"Tuning Ridge Classifier...")
    grid_search.fit(X_train, y_train)

    # To get best model and ALL parameters.
    best_model = grid_search.best_estimator_
    best_params = grid_search.best_params_

    print(f"Best Parameters Found:")
    print(f"  alpha: {best_params.get('alpha')}")
    print(f"  solver: {best_params.get('solver')}")
    print(f"  fit_intercept: {best_params.get('fit_intercept')}")
    print(f"  copy_X: {best_params.get('copy_X')}")
    print(f"  class_weight: {best_params.get('class_weight')}")
    print(f"  random_state: {best_params.get('random_state')}")
    print(f"  Best CV Score: {grid_search.best_score_:.4f}")

    # To use the generic training pipeline with tuned model.
    model, cv_scores, results = train_model_generic(best_model, X_train, y_train,
                                                    X_test, y_test, "Tuned Ridge Classifier")

    return model, cv_scores, results

ridge_model, ridge_cv, ridge_results = tune_ridge_classifier(X_train, y_train, X_test, y_test)
results_dict['Ridge Classifier'] = {'model': ridge_model,
                                    'cv_scores': ridge_cv,
                                    'results': ridge_results}

"""- Random forest"""

def train_random_forest(X_train, y_train, X_test, y_test):

    """
    Refactored version using generic functions for Random Forest.
    Uses conservative parameters to prevent overfitting for time series data.

    Parameters: X_train, y_train : Training data.
                X_test, y_test : Test data.

    Returns: model : Trained Random Forest model.
             cv_scores : Cross-validation scores.
             results : Test evaluation results.
    """

    # To initialize the Ridge Classifier model.
    model = RandomForestClassifier(n_estimators=50,
                                   max_depth=3,
                                   min_samples_split=30,
                                   min_samples_leaf=15,
                                   max_features=0.3,
                                   max_samples=0.7,
                                   bootstrap=True,
                                   oob_score=True,
                                   random_state=42,
                                   n_jobs=-1,
                                   verbose=0)

    # To use the generic training pipeline.
    model, cv_scores, results = train_model_generic(model, X_train, y_train,
                                                    X_test, y_test, "Random Forest")

    return model, cv_scores, results

model_rf, cv_rf, results_rf = train_random_forest(X_train_scaled, y_train,
                                                  X_test_scaled, y_test)

def tune_random_forest(X_train, y_train, X_test, y_test):

    """
    This function performs hyperparameter tuning for Random Forest.

    Parameters: X_train, y_train : Training data.
                X_test, y_test : Test data.
    Returns: best_model : Best Random Forest model.
             cv_scores : Cross-validation scores.
             results : Test evaluation results.
    """

    # To define the parameter grid for key Random Forest parameters.
    param_grid = {'n_estimators': [100, 200, 300],
                  'max_depth': [3, 4, 5],
                  'min_samples_split': [5, 10, 15],
                  'min_samples_leaf': [2, 3, 4],
                  'max_features': [0.2],
                  'criterion': ['entropy'],
                  'class_weight': ['balanced', {0: 1, 1: 1.1}, {0: 1.1, 1: 1}]}

    # To configure time series cross-validation.
    tscv = TimeSeriesSplit(n_splits=5)

    # To create GridSearchCV for hyperparameter tuning.
    grid_search = GridSearchCV(estimator=RandomForestClassifier(random_state=42),
                               param_grid=param_grid,
                               cv=tscv,
                               scoring='balanced_accuracy',
                               n_jobs=-1,
                               verbose=1,
                               refit=True)

    # To execute grid search.
    print(f"Tuning Random Forest...")
    grid_search.fit(X_train, y_train)

    # To get best model and ALL parameters.
    best_model = grid_search.best_estimator_
    best_params = grid_search.best_params_

    print(f"Best Parameters Found:")
    print(f"  n_estimators: {best_params.get('n_estimators')}")
    print(f"  max_depth: {best_params.get('max_depth')}")
    print(f"  min_samples_split: {best_params.get('min_samples_split')}")
    print(f"  min_samples_leaf: {best_params.get('min_samples_leaf')}")
    print(f"  max_features: {best_params.get('max_features')}")
    print(f"  class_weight: {best_params.get('class_weight')}")
    print(f"Best CV Score: {grid_search.best_score_:.4f}")

    # To use the generic training pipeline with tuned model.
    model, cv_scores, results = train_model_generic(best_model, X_train, y_train,
                                                    X_test, y_test, "Tuned Random Forest")

    return model, cv_scores, results

rf_model, rf_cv, rf_results = tune_random_forest(X_train, y_train, X_test, y_test)
results_dict['Random Forest'] = {'model': rf_model,
                                 'cv_scores': rf_cv,
                                 'results': rf_results}